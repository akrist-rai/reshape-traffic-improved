
## ğŸ”¹ What is Heterophily? 

### Homophily (GCN / GAT assumption)

> â€œFriends are similarâ€

* Same class neighbors
* Feature smoothing works

### Heterophily (Real-world case)

> â€œOpposites attractâ€

* Fraud â†” normal users
* Malicious â†” benign nodes
* Traffic bottleneck â†” free-flow neighbors

ğŸ“‰ Standard GAT **fails** here due to over-smoothing.

---

## ğŸ”¹ Core Problem with Normal GAT

GAT aggregates neighbors like:

> h<sub>i</sub><sup>new</sup> = âˆ‘<sub>j âˆˆ ğ’©(i)</sub> Î±<sub>ij</sub> h<sub>j</sub>


But under heterophily:

* Neighbors belong to **different classes**
* Aggregation **destroys node identity**

---

## ğŸ”¹ How HA-GAT Fixes This

### **1ï¸âƒ£ Separates Self vs Neighbor Information**

HA-GAT **does not blindly mix** neighbors.


> h<sub>i</sub><sup>new</sup> = Î» h<sub>i</sub> + âˆ‘<sub>j</sub> Î±<sub>ij</sub> h<sub>j</sub>


* Self-node kept dominant
* Prevents feature dilution

---

### **2ï¸âƒ£ Signed / Directional Attention**

Instead of â€œimportant vs not importantâ€, HA-GAT learns:

* **Helpful neighbors**
* **Harmful neighbors**


> Î±<sub>ij</sub> âˆˆ [âˆ’1, +1]

Negative attention = **repulsion**, not attraction.

---

### **3ï¸âƒ£ Higher-Order Neighborhood Mixing**

Heterophily often appears at **2-hop or 3-hop** distance.

HA-GAT combines:

* 1-hop (different)
* 2-hop (often similar!)

> h<sub>i</sub><sup>final</sup> = h<sub>i</sub><sup>(1)</sup> + h<sub>i</sub><sup>(2)</sup>



---

### **4ï¸âƒ£ Feature-wise Attention (Key Upgrade)**

Instead of node-level only:

* Attention applied **per feature channel**
* Some features attract, others repel

---

## ğŸ”¹ Architecture Overview

```
Node Features
   â†“
Self-Embedding (Strong)
   â†“
Signed Attention (Â±)
   â†“
Multi-hop Aggregation
   â†“
Classifier
```

---

## ğŸ”¹ HA-GAT vs GAT vs GCN

| Property                  | GCN  | GAT    | HA-GAT |
| ------------------------- | ---- | ------ | ------ |
| Assumes homophily         | âœ…    | âœ…      | âŒ      |
| Handles heterophily       | âŒ    | âŒ      | âœ…      |
| Signed attention          | âŒ    | âŒ      | âœ…      |
| Self-feature preservation | Weak | Medium | Strong |
| Real-world robustness     | Low  | Medium | High   |

---

