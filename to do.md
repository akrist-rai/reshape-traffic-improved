
* RTX 3050 Laptop
* **4 GB VRAM**
* **HA-GAT instead of GAT**
* **Multi-day training allowed**
* Ubuntu / WSL2 / Colab compatible


---

# ğŸš¦ Spatio-Temporal Traffic Prediction

**(ST-Mamba + HA-GAT | Low-VRAM Training Setup)**

This project implements a **spatio-temporal traffic forecasting system** using:

* **ST-Mamba** for temporal modeling
* **HA-GAT (Heterophily-Aware GAT)** for spatial graph learning

The project is optimized to **run on low-VRAM GPUs (RTX 3050 â€“ 4GB)** using:

* Small batch sizes
* Gradient accumulation
* Mixed precision (FP16)
* Multi-day training

---

## ğŸ–¥ï¸ Target Hardware

| Component | Spec            |
| --------- | --------------- |
| GPU       | RTX 3050 Laptop |
| VRAM      | 4 GB            |
| RAM       | 8â€“16 GB         |
| OS        | Ubuntu / WSL2   |
| CUDA      | 12.x            |
| PyTorch   | CUDA-enabled    |

---

## ğŸ“ Project Structure

```text
project/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ st_mamba.py          # Temporal model
â”‚   â”œâ”€â”€ ha_gat.py            # HA-GAT implementation
â”‚   â””â”€â”€ full_model.py        # Combined ST-Mamba + HA-GAT
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ traffic_dataset.py   # Dataset loader
â”‚   â””â”€â”€ preprocess.py        # Graph + time preprocessing
â”‚
â”œâ”€â”€ train.py                 # MAIN training file (run this)
â”œâ”€â”€ eval.py                  # Evaluation script
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ low_vram.yaml        # RTX 3050 safe config
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ seed.py
â”‚   â””â”€â”€ checkpoint.py
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ latest.pt
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… TO-DO LIST (IN ORDER)

### ğŸ”¹ Phase 1 â€” Environment Setup

* [ ] Install NVIDIA drivers on Windows
* [ ] Enable WSL2 + Ubuntu
* [ ] Install CUDA PyTorch inside WSL
* [ ] Verify GPU visibility (`nvidia-smi`)

---

### ğŸ”¹ Phase 2 â€” Model Preparation

* [ ] Replace **GAT â†’ HA-GAT**
* [ ] Limit HA-GAT attention heads (â‰¤ 2)
* [ ] Use sparse adjacency (NO dense NÃ—N attention)
* [ ] Cap neighbors per node (â‰¤ 15)

---

### ğŸ”¹ Phase 3 â€” Low-VRAM Training Configuration

* [ ] Reduce batch size to **1**
* [ ] Enable **FP16 (AMP)**
* [ ] Enable gradient accumulation
* [ ] Reduce temporal window if needed
* [ ] Enable gradient clipping

---

### ğŸ”¹ Phase 4 â€” Fault-Tolerant Training

* [ ] Enable checkpoint saving every 1k steps
* [ ] Enable resume-from-checkpoint
* [ ] Log training loss & metrics
* [ ] Monitor GPU memory usage

---

### ğŸ”¹ Phase 5 â€” Evaluation

* [ ] Load best checkpoint
* [ ] Run eval.py
* [ ] Export metrics (MAE, RMSE)
* [ ] Save predictions

---

## âš™ï¸ Low-VRAM Safe Configuration

**configs/low_vram.yaml**

```yaml
batch_size: 1
grad_accum_steps: 32        # effective batch = 32
learning_rate: 0.0001
weight_decay: 0.0001

num_nodes: 300
seq_len: 48
pred_len: 12

ha_gat:
  hidden_dim: 64
  num_heads: 2
  num_layers: 2
  max_neighbors: 15
  attention_dropout: 0.2

training:
  use_amp: true
  grad_clip: 1.0
  checkpoint_interval: 1000
```

---

## â–¶ï¸ How to RUN (RTX 3050 â€“ 4GB)

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Start training (MAIN ENTRY POINT)

```bash
python train.py --config configs/low_vram.yaml
```

> â± Training may take **days** â€” this is expected and correct.

---

## ğŸ§  Training Strategy (IMPORTANT)

* **Batch size = 1** is intentional
* Large batches are simulated using **gradient accumulation**
* Multi-day training is **stable and correct**
* HA-GAT works well with small batches

---

## ğŸ’¾ Checkpointing & Resume

Checkpoints are saved in:

```text
checkpoints/latest.pt
```

Resume training:

```bash
python train.py --resume checkpoints/latest.pt
```

---

## ğŸ“Š Evaluation

```bash
python eval.py --checkpoint checkpoints/best.pt
```

Metrics:

* MAE
* RMSE
* MAPE

---

## ğŸš€ When to Use Google Colab

Use Colab **only if**:

* Nodes > 500
* Seq length > 96
* Batch size > 2
* Final large-scale training

Otherwise, **RTX 3050 + patience is enough**.

---

## âš ï¸ Common Mistakes (DO NOT DO)

âŒ Dense adjacency matrices
âŒ Batch size > 2
âŒ FP32 training
âŒ No checkpointing
âŒ High learning rate

---

## âœ… Final Notes

* This setup is **research-grade**
* Training is slow but **correct**
* HA-GAT improves heterophilic graph learning
* Time is traded for memory â€” intentionally


